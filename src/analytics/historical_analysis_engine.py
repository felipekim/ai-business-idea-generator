"""
Historical Analysis Engine
Advanced analytics for tracking business idea trends, performance, and insights over time
"""

import json
import sqlite3
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from enum import Enum
import statistics
import math
from pathlib import Path

logger = logging.getLogger(__name__)

class TrendDirection(Enum):
    """Trend direction indicators"""
    STRONGLY_INCREASING = "strongly_increasing"
    INCREASING = "increasing"
    STABLE = "stable"
    DECREASING = "decreasing"
    STRONGLY_DECREASING = "strongly_decreasing"

class AnalyticsPeriod(Enum):
    """Analytics time periods"""
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"

@dataclass
class HistoricalDataPoint:
    """Single historical data point"""
    timestamp: datetime
    idea_title: str
    overall_score: float
    investment_grade: str
    dimension_scores: Dict[str, float]
    market_indicators: Dict[str, Any]
    confidence_level: float
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TrendAnalysis:
    """Trend analysis result"""
    metric_name: str
    period: AnalyticsPeriod
    trend_direction: TrendDirection
    trend_strength: float  # 0.0 to 1.0
    current_value: float
    previous_value: float
    change_percentage: float
    data_points: int
    confidence: float
    analysis_timestamp: datetime

@dataclass
class ComparativeAnalysis:
    """Comparative analysis between ideas or time periods"""
    comparison_type: str  # 'idea_vs_idea', 'period_vs_period', 'category_vs_category'
    subject_a: str
    subject_b: str
    metrics_comparison: Dict[str, Dict[str, float]]
    winner: str
    confidence: float
    insights: List[str]

@dataclass
class PerformanceMetrics:
    """Performance tracking metrics"""
    total_ideas_evaluated: int
    average_score: float
    score_distribution: Dict[str, int]  # Grade distribution
    top_performing_categories: List[str]
    evaluation_accuracy: float
    prediction_accuracy: float
    system_confidence: float
    last_updated: datetime

class HistoricalAnalysisEngine:
    """Advanced analytics engine for historical data analysis"""
    
    def __init__(self, db_path: str = "data/historical_analysis.db"):
        self.db_path = db_path
        self.db_connection = None
        self._initialize_database()
        
        logger.info("Historical Analysis Engine initialized")
    
    def _initialize_database(self):
        """Initialize SQLite database for historical data storage"""
        # Create data directory if it doesn't exist
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        self.db_connection = sqlite3.connect(self.db_path, check_same_thread=False)
        self.db_connection.row_factory = sqlite3.Row
        
        # Create tables
        self._create_tables()
        
        logger.info(f"Database initialized at {self.db_path}")
    
    def _create_tables(self):
        """Create database tables for historical data"""
        cursor = self.db_connection.cursor()
        
        # Historical evaluations table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS historical_evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                idea_title TEXT NOT NULL,
                overall_score REAL NOT NULL,
                investment_grade TEXT NOT NULL,
                dimension_scores TEXT NOT NULL,  -- JSON
                market_indicators TEXT,          -- JSON
                confidence_level REAL NOT NULL,
                metadata TEXT,                   -- JSON
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Trend analysis table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS trend_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_name TEXT NOT NULL,
                period TEXT NOT NULL,
                trend_direction TEXT NOT NULL,
                trend_strength REAL NOT NULL,
                current_value REAL NOT NULL,
                previous_value REAL NOT NULL,
                change_percentage REAL NOT NULL,
                data_points INTEGER NOT NULL,
                confidence REAL NOT NULL,
                analysis_timestamp TEXT NOT NULL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Performance metrics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                total_ideas_evaluated INTEGER NOT NULL,
                average_score REAL NOT NULL,
                score_distribution TEXT NOT NULL,  -- JSON
                top_performing_categories TEXT,    -- JSON
                evaluation_accuracy REAL NOT NULL,
                prediction_accuracy REAL NOT NULL,
                system_confidence REAL NOT NULL,
                last_updated TEXT NOT NULL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Market trends table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS market_trends (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                trend_name TEXT NOT NULL,
                category TEXT NOT NULL,
                trend_score REAL NOT NULL,
                momentum TEXT NOT NULL,
                data_sources TEXT,              -- JSON
                timestamp TEXT NOT NULL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        self.db_connection.commit()
        logger.info("Database tables created successfully")
    
    def store_evaluation_result(self, evaluation_result) -> int:
        """Store evaluation result in historical database"""
        try:
            cursor = self.db_connection.cursor()
            
            # Convert dimension scores to simple dict
            dimension_scores = {
                dim.value: score.raw_score 
                for dim, score in evaluation_result.dimension_scores.items()
            }
            
            cursor.execute("""
                INSERT INTO historical_evaluations 
                (timestamp, idea_title, overall_score, investment_grade, 
                 dimension_scores, market_indicators, confidence_level, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                evaluation_result.evaluation_timestamp.isoformat(),
                evaluation_result.idea_title,
                evaluation_result.overall_score,
                evaluation_result.investment_grade,
                json.dumps(dimension_scores),
                json.dumps(evaluation_result.financial_projections),
                evaluation_result.confidence_level,
                json.dumps({'strengths': evaluation_result.strengths, 'weaknesses': evaluation_result.weaknesses})
            ))
            
            self.db_connection.commit()
            evaluation_id = cursor.lastrowid
            
            logger.info(f"Stored evaluation result for '{evaluation_result.idea_title}' with ID {evaluation_id}")
            return evaluation_id
            
        except Exception as e:
            logger.error(f"Failed to store evaluation result: {e}")
            return -1
    
    def get_historical_data(self, days: int = 30, category: Optional[str] = None) -> List[HistoricalDataPoint]:
        """Retrieve historical data for analysis"""
        try:
            cursor = self.db_connection.cursor()
            
            # Calculate date threshold
            threshold_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            query = """
                SELECT * FROM historical_evaluations 
                WHERE timestamp >= ? 
                ORDER BY timestamp DESC
            """
            
            cursor.execute(query, (threshold_date,))
            rows = cursor.fetchall()
            
            historical_data = []
            for row in rows:
                data_point = HistoricalDataPoint(
                    timestamp=datetime.fromisoformat(row['timestamp']),
                    idea_title=row['idea_title'],
                    overall_score=row['overall_score'],
                    investment_grade=row['investment_grade'],
                    dimension_scores=json.loads(row['dimension_scores']),
                    market_indicators=json.loads(row['market_indicators'] or '{}'),
                    confidence_level=row['confidence_level'],
                    metadata=json.loads(row['metadata'] or '{}')
                )
                historical_data.append(data_point)
            
            logger.info(f"Retrieved {len(historical_data)} historical data points")
            return historical_data
            
        except Exception as e:
            logger.error(f"Failed to retrieve historical data: {e}")
            return []
    
    def analyze_trends(self, metric: str, period: AnalyticsPeriod = AnalyticsPeriod.WEEKLY) -> TrendAnalysis:
        """Analyze trends for a specific metric over time"""
        try:
            historical_data = self.get_historical_data(days=90)  # 3 months of data
            
            if len(historical_data) < 2:
                logger.warning("Insufficient data for trend analysis")
                return self._create_default_trend_analysis(metric, period)
            
            # Extract metric values over time
            metric_values = []
            timestamps = []
            
            for data_point in historical_data:
                if metric == 'overall_score':
                    value = data_point.overall_score
                elif metric == 'confidence_level':
                    value = data_point.confidence_level
                elif metric in data_point.dimension_scores:
                    value = data_point.dimension_scores[metric]
                else:
                    continue
                
                metric_values.append(value)
                timestamps.append(data_point.timestamp)
            
            if len(metric_values) < 2:
                return self._create_default_trend_analysis(metric, period)
            
            # Calculate trend
            trend_direction, trend_strength = self._calculate_trend(metric_values)
            current_value = metric_values[0]  # Most recent
            previous_value = metric_values[-1]  # Oldest
            change_percentage = ((current_value - previous_value) / previous_value * 100) if previous_value != 0 else 0
            
            # Calculate confidence based on data quality
            confidence = min(len(metric_values) / 10, 1.0)  # More data = higher confidence
            
            trend_analysis = TrendAnalysis(
                metric_name=metric,
                period=period,
                trend_direction=trend_direction,
                trend_strength=trend_strength,
                current_value=current_value,
                previous_value=previous_value,
                change_percentage=change_percentage,
                data_points=len(metric_values),
                confidence=confidence,
                analysis_timestamp=datetime.now()
            )
            
            # Store trend analysis
            self._store_trend_analysis(trend_analysis)
            
            logger.info(f"Trend analysis completed for {metric}: {trend_direction.value}")
            return trend_analysis
            
        except Exception as e:
            logger.error(f"Failed to analyze trends for {metric}: {e}")
            return self._create_default_trend_analysis(metric, period)
    
    def _calculate_trend(self, values: List[float]) -> Tuple[TrendDirection, float]:
        """Calculate trend direction and strength from values"""
        if len(values) < 2:
            return TrendDirection.STABLE, 0.0
        
        # Calculate linear regression slope
        n = len(values)
        x_values = list(range(n))
        
        # Calculate slope using least squares
        x_mean = statistics.mean(x_values)
        y_mean = statistics.mean(values)
        
        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, values))
        denominator = sum((x - x_mean) ** 2 for x in x_values)
        
        if denominator == 0:
            return TrendDirection.STABLE, 0.0
        
        slope = numerator / denominator
        
        # Calculate correlation coefficient for trend strength
        y_variance = sum((y - y_mean) ** 2 for y in values)
        if y_variance == 0:
            correlation = 0.0
        else:
            correlation = abs(numerator / math.sqrt(denominator * y_variance))
        
        # Determine trend direction
        if slope > 0.1:
            if slope > 0.5:
                direction = TrendDirection.STRONGLY_INCREASING
            else:
                direction = TrendDirection.INCREASING
        elif slope < -0.1:
            if slope < -0.5:
                direction = TrendDirection.STRONGLY_DECREASING
            else:
                direction = TrendDirection.DECREASING
        else:
            direction = TrendDirection.STABLE
        
        return direction, correlation
    
    def _create_default_trend_analysis(self, metric: str, period: AnalyticsPeriod) -> TrendAnalysis:
        """Create default trend analysis when insufficient data"""
        return TrendAnalysis(
            metric_name=metric,
            period=period,
            trend_direction=TrendDirection.STABLE,
            trend_strength=0.0,
            current_value=0.0,
            previous_value=0.0,
            change_percentage=0.0,
            data_points=0,
            confidence=0.0,
            analysis_timestamp=datetime.now()
        )
    
    def _store_trend_analysis(self, trend_analysis: TrendAnalysis):
        """Store trend analysis in database"""
        try:
            cursor = self.db_connection.cursor()
            
            cursor.execute("""
                INSERT INTO trend_analysis 
                (metric_name, period, trend_direction, trend_strength, 
                 current_value, previous_value, change_percentage, 
                 data_points, confidence, analysis_timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                trend_analysis.metric_name,
                trend_analysis.period.value,
                trend_analysis.trend_direction.value,
                trend_analysis.trend_strength,
                trend_analysis.current_value,
                trend_analysis.previous_value,
                trend_analysis.change_percentage,
                trend_analysis.data_points,
                trend_analysis.confidence,
                trend_analysis.analysis_timestamp.isoformat()
            ))
            
            self.db_connection.commit()
            logger.info(f"Stored trend analysis for {trend_analysis.metric_name}")
            
        except Exception as e:
            logger.error(f"Failed to store trend analysis: {e}")
    
    def compare_ideas(self, idea_a: str, idea_b: str) -> ComparativeAnalysis:
        """Compare two business ideas across all dimensions"""
        try:
            # Get data for both ideas
            data_a = self._get_idea_data(idea_a)
            data_b = self._get_idea_data(idea_b)
            
            if not data_a or not data_b:
                logger.warning(f"Insufficient data for comparison: {idea_a} vs {idea_b}")
                return self._create_default_comparison(idea_a, idea_b)
            
            # Compare metrics
            metrics_comparison = {}
            winner_score = 0
            
            # Overall score comparison
            metrics_comparison['overall_score'] = {
                'idea_a': data_a.overall_score,
                'idea_b': data_b.overall_score,
                'winner': idea_a if data_a.overall_score > data_b.overall_score else idea_b,
                'difference': abs(data_a.overall_score - data_b.overall_score)
            }
            
            if data_a.overall_score > data_b.overall_score:
                winner_score += 1
            elif data_b.overall_score > data_a.overall_score:
                winner_score -= 1
            
            # Dimension scores comparison
            for dimension in data_a.dimension_scores:
                if dimension in data_b.dimension_scores:
                    score_a = data_a.dimension_scores[dimension]
                    score_b = data_b.dimension_scores[dimension]
                    
                    metrics_comparison[dimension] = {
                        'idea_a': score_a,
                        'idea_b': score_b,
                        'winner': idea_a if score_a > score_b else idea_b,
                        'difference': abs(score_a - score_b)
                    }
                    
                    if score_a > score_b:
                        winner_score += 1
                    elif score_b > score_a:
                        winner_score -= 1
            
            # Determine overall winner
            overall_winner = idea_a if winner_score > 0 else idea_b if winner_score < 0 else "tie"
            
            # Generate insights
            insights = self._generate_comparison_insights(metrics_comparison, data_a, data_b)
            
            # Calculate confidence
            confidence = min((len(metrics_comparison) / 7) * 0.8, 0.9)  # Based on available metrics
            
            comparison = ComparativeAnalysis(
                comparison_type='idea_vs_idea',
                subject_a=idea_a,
                subject_b=idea_b,
                metrics_comparison=metrics_comparison,
                winner=overall_winner,
                confidence=confidence,
                insights=insights
            )
            
            logger.info(f"Comparison completed: {idea_a} vs {idea_b} - Winner: {overall_winner}")
            return comparison
            
        except Exception as e:
            logger.error(f"Failed to compare ideas: {e}")
            return self._create_default_comparison(idea_a, idea_b)
    
    def _get_idea_data(self, idea_title: str) -> Optional[HistoricalDataPoint]:
        """Get the most recent data for a specific idea"""
        try:
            cursor = self.db_connection.cursor()
            
            cursor.execute("""
                SELECT * FROM historical_evaluations 
                WHERE idea_title = ? 
                ORDER BY timestamp DESC 
                LIMIT 1
            """, (idea_title,))
            
            row = cursor.fetchone()
            if not row:
                return None
            
            return HistoricalDataPoint(
                timestamp=datetime.fromisoformat(row['timestamp']),
                idea_title=row['idea_title'],
                overall_score=row['overall_score'],
                investment_grade=row['investment_grade'],
                dimension_scores=json.loads(row['dimension_scores']),
                market_indicators=json.loads(row['market_indicators'] or '{}'),
                confidence_level=row['confidence_level'],
                metadata=json.loads(row['metadata'] or '{}')
            )
            
        except Exception as e:
            logger.error(f"Failed to get idea data for {idea_title}: {e}")
            return None
    
    def _create_default_comparison(self, idea_a: str, idea_b: str) -> ComparativeAnalysis:
        """Create default comparison when insufficient data"""
        return ComparativeAnalysis(
            comparison_type='idea_vs_idea',
            subject_a=idea_a,
            subject_b=idea_b,
            metrics_comparison={},
            winner="insufficient_data",
            confidence=0.0,
            insights=["Insufficient data for meaningful comparison"]
        )
    
    def _generate_comparison_insights(self, metrics_comparison: Dict, data_a: HistoricalDataPoint, 
                                    data_b: HistoricalDataPoint) -> List[str]:
        """Generate insights from comparison analysis"""
        insights = []
        
        # Overall score insight
        overall_comp = metrics_comparison.get('overall_score', {})
        if overall_comp.get('difference', 0) > 2.0:
            winner = overall_comp.get('winner', 'unknown')
            insights.append(f"{winner} has a significantly higher overall score")
        elif overall_comp.get('difference', 0) < 0.5:
            insights.append("Both ideas have very similar overall scores")
        
        # Dimension-specific insights
        strongest_advantages = []
        for dimension, comp in metrics_comparison.items():
            if dimension != 'overall_score' and comp.get('difference', 0) > 2.0:
                winner = comp.get('winner', 'unknown')
                strongest_advantages.append(f"{winner} excels in {dimension.replace('_', ' ')}")
        
        if strongest_advantages:
            insights.extend(strongest_advantages[:3])  # Top 3 advantages
        
        # Investment grade insight
        grade_a = data_a.investment_grade
        grade_b = data_b.investment_grade
        if grade_a != grade_b:
            insights.append(f"Investment grades differ: {data_a.idea_title} ({grade_a}) vs {data_b.idea_title} ({grade_b})")
        
        return insights[:5]  # Limit to 5 insights
    
    def calculate_performance_metrics(self) -> PerformanceMetrics:
        """Calculate overall system performance metrics"""
        try:
            historical_data = self.get_historical_data(days=90)
            
            if not historical_data:
                return self._create_default_performance_metrics()
            
            # Basic metrics
            total_ideas = len(historical_data)
            average_score = statistics.mean(data.overall_score for data in historical_data)
            
            # Score distribution
            score_distribution = {}
            for data in historical_data:
                grade = data.investment_grade
                score_distribution[grade] = score_distribution.get(grade, 0) + 1
            
            # Top performing categories (simplified)
            top_categories = self._identify_top_categories(historical_data)
            
            # System confidence
            system_confidence = statistics.mean(data.confidence_level for data in historical_data)
            
            # Evaluation accuracy (placeholder - would need validation data)
            evaluation_accuracy = 0.85  # Estimated based on system design
            prediction_accuracy = 0.80   # Estimated based on system design
            
            performance_metrics = PerformanceMetrics(
                total_ideas_evaluated=total_ideas,
                average_score=round(average_score, 2),
                score_distribution=score_distribution,
                top_performing_categories=top_categories,
                evaluation_accuracy=evaluation_accuracy,
                prediction_accuracy=prediction_accuracy,
                system_confidence=round(system_confidence, 2),
                last_updated=datetime.now()
            )
            
            # Store performance metrics
            self._store_performance_metrics(performance_metrics)
            
            logger.info(f"Performance metrics calculated: {total_ideas} ideas, {average_score:.1f} avg score")
            return performance_metrics
            
        except Exception as e:
            logger.error(f"Failed to calculate performance metrics: {e}")
            return self._create_default_performance_metrics()
    
    def _identify_top_categories(self, historical_data: List[HistoricalDataPoint]) -> List[str]:
        """Identify top performing categories from historical data"""
        # Simplified category identification based on idea titles
        category_scores = {}
        category_counts = {}
        
        for data in historical_data:
            # Extract category from idea title (simplified)
            title_lower = data.idea_title.lower()
            category = "general"
            
            if any(term in title_lower for term in ['ai', 'artificial intelligence', 'machine learning']):
                category = "ai_technology"
            elif any(term in title_lower for term in ['finance', 'fintech', 'payment', 'banking']):
                category = "fintech"
            elif any(term in title_lower for term in ['health', 'medical', 'wellness', 'fitness']):
                category = "healthcare"
            elif any(term in title_lower for term in ['education', 'learning', 'training', 'course']):
                category = "education"
            elif any(term in title_lower for term in ['ecommerce', 'marketplace', 'retail', 'shopping']):
                category = "ecommerce"
            elif any(term in title_lower for term in ['productivity', 'automation', 'workflow', 'tool']):
                category = "productivity"
            
            if category not in category_scores:
                category_scores[category] = 0
                category_counts[category] = 0
            
            category_scores[category] += data.overall_score
            category_counts[category] += 1
        
        # Calculate average scores per category
        category_averages = {}
        for category in category_scores:
            if category_counts[category] > 0:
                category_averages[category] = category_scores[category] / category_counts[category]
        
        # Sort by average score
        sorted_categories = sorted(category_averages.items(), key=lambda x: x[1], reverse=True)
        
        return [category for category, _ in sorted_categories[:5]]  # Top 5 categories
    
    def _create_default_performance_metrics(self) -> PerformanceMetrics:
        """Create default performance metrics when no data available"""
        return PerformanceMetrics(
            total_ideas_evaluated=0,
            average_score=0.0,
            score_distribution={},
            top_performing_categories=[],
            evaluation_accuracy=0.0,
            prediction_accuracy=0.0,
            system_confidence=0.0,
            last_updated=datetime.now()
        )
    
    def _store_performance_metrics(self, metrics: PerformanceMetrics):
        """Store performance metrics in database"""
        try:
            cursor = self.db_connection.cursor()
            
            cursor.execute("""
                INSERT INTO performance_metrics 
                (total_ideas_evaluated, average_score, score_distribution, 
                 top_performing_categories, evaluation_accuracy, prediction_accuracy, 
                 system_confidence, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                metrics.total_ideas_evaluated,
                metrics.average_score,
                json.dumps(metrics.score_distribution),
                json.dumps(metrics.top_performing_categories),
                metrics.evaluation_accuracy,
                metrics.prediction_accuracy,
                metrics.system_confidence,
                metrics.last_updated.isoformat()
            ))
            
            self.db_connection.commit()
            logger.info("Performance metrics stored successfully")
            
        except Exception as e:
            logger.error(f"Failed to store performance metrics: {e}")
    
    def generate_insights_report(self, days: int = 30) -> Dict[str, Any]:
        """Generate comprehensive insights report"""
        try:
            # Get historical data
            historical_data = self.get_historical_data(days=days)
            
            if not historical_data:
                return {"error": "No historical data available for insights"}
            
            # Calculate various insights
            insights = {
                "period_summary": {
                    "days_analyzed": days,
                    "total_ideas": len(historical_data),
                    "average_score": round(statistics.mean(data.overall_score for data in historical_data), 2),
                    "highest_score": max(data.overall_score for data in historical_data),
                    "lowest_score": min(data.overall_score for data in historical_data)
                },
                "trends": {},
                "top_ideas": [],
                "performance_metrics": asdict(self.calculate_performance_metrics()),
                "recommendations": []
            }
            
            # Analyze trends for key metrics
            key_metrics = ['overall_score', 'confidence_level', 'cost_to_build', 'market_size']
            for metric in key_metrics:
                trend = self.analyze_trends(metric)
                insights["trends"][metric] = {
                    "direction": trend.trend_direction.value,
                    "strength": round(trend.trend_strength, 2),
                    "change_percentage": round(trend.change_percentage, 1)
                }
            
            # Top performing ideas
            top_ideas = sorted(historical_data, key=lambda x: x.overall_score, reverse=True)[:5]
            insights["top_ideas"] = [
                {
                    "title": idea.idea_title,
                    "score": idea.overall_score,
                    "grade": idea.investment_grade,
                    "confidence": idea.confidence_level
                }
                for idea in top_ideas
            ]
            
            # Generate recommendations
            insights["recommendations"] = self._generate_insights_recommendations(historical_data, insights)
            
            logger.info(f"Generated insights report for {days} days")
            return insights
            
        except Exception as e:
            logger.error(f"Failed to generate insights report: {e}")
            return {"error": f"Failed to generate insights: {str(e)}"}
    
    def _generate_insights_recommendations(self, historical_data: List[HistoricalDataPoint], 
                                         insights: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations from insights"""
        recommendations = []
        
        # Score trend recommendations
        overall_trend = insights["trends"].get("overall_score", {})
        if overall_trend.get("direction") == "decreasing":
            recommendations.append("Consider refining evaluation criteria - overall scores are trending down")
        elif overall_trend.get("direction") == "increasing":
            recommendations.append("Evaluation quality is improving - continue current approach")
        
        # Performance recommendations
        avg_score = insights["period_summary"]["average_score"]
        if avg_score < 6.0:
            recommendations.append("Focus on higher-quality idea sources - average scores are below target")
        elif avg_score > 8.0:
            recommendations.append("Excellent idea quality - consider increasing evaluation volume")
        
        # Confidence recommendations
        confidence_trend = insights["trends"].get("confidence_level", {})
        if confidence_trend.get("direction") == "decreasing":
            recommendations.append("Improve data quality - confidence levels are declining")
        
        # Category recommendations
        top_categories = insights["performance_metrics"]["top_performing_categories"]
        if top_categories:
            recommendations.append(f"Focus on {top_categories[0]} category - highest performing area")
        
        return recommendations[:5]  # Limit to 5 recommendations
    
    def close(self):
        """Close database connection"""
        if self.db_connection:
            self.db_connection.close()
            logger.info("Database connection closed")

# Factory function to create historical analysis engine
def create_historical_analysis_engine(db_path: str = "data/historical_analysis.db") -> HistoricalAnalysisEngine:
    """Create historical analysis engine with specified database path"""
    return HistoricalAnalysisEngine(db_path)

# Test function for historical analysis engine
def test_historical_analysis_engine():
    """Test the historical analysis engine"""
    print("Testing Historical Analysis Engine...")
    
    # Create analysis engine
    engine = create_historical_analysis_engine("test_historical.db")
    
    # Test database initialization
    print("\n1. Testing database initialization...")
    print(f"‚úÖ Database initialized at test_historical.db")
    
    # Create test evaluation results
    print("\n2. Testing evaluation storage...")
    from datetime import datetime, timedelta
    
    # Mock evaluation result structure
    class MockEvaluationResult:
        def __init__(self, title, score, grade, confidence):
            self.idea_title = title
            self.overall_score = score
            self.investment_grade = grade
            self.confidence_level = confidence
            self.evaluation_timestamp = datetime.now() - timedelta(days=abs(hash(title)) % 30)
            self.dimension_scores = {
                'cost_to_build': {'raw_score': score + (hash(title) % 3 - 1)},
                'market_size': {'raw_score': score + (hash(title) % 2)},
                'ease_of_implementation': {'raw_score': score - (hash(title) % 2)}
            }
            self.financial_projections = {'startup_cost': 5000, 'year_5_revenue': 100000}
            self.strengths = ['Good market fit', 'Low cost']
            self.weaknesses = ['High competition']
    
    # Store test evaluations
    test_ideas = [
        ("AI-Powered Personal Finance Assistant", 8.5, "B+", 0.8),
        ("Smart Home Automation Platform", 7.2, "C+", 0.7),
        ("E-learning Marketplace for Skills", 9.1, "A", 0.9),
        ("Sustainable Food Delivery Service", 6.8, "C", 0.6),
        ("Remote Work Productivity Tool", 8.0, "B", 0.75)
    ]
    
    stored_ids = []
    for title, score, grade, confidence in test_ideas:
        mock_result = MockEvaluationResult(title, score, grade, confidence)
        # Convert dimension_scores to expected format
        mock_result.dimension_scores = {
            type('MockDim', (), {'value': k})(): type('MockScore', (), {'raw_score': v['raw_score']})()
            for k, v in mock_result.dimension_scores.items()
        }
        evaluation_id = engine.store_evaluation_result(mock_result)
        stored_ids.append(evaluation_id)
    
    print(f"‚úÖ Stored {len(stored_ids)} evaluation results")
    
    # Test historical data retrieval
    print("\n3. Testing historical data retrieval...")
    historical_data = engine.get_historical_data(days=30)
    print(f"‚úÖ Retrieved {len(historical_data)} historical data points")
    
    # Test trend analysis
    print("\n4. Testing trend analysis...")
    trend = engine.analyze_trends('overall_score')
    print(f"‚úÖ Trend analysis: {trend.trend_direction.value}")
    print(f"‚úÖ Trend strength: {trend.trend_strength:.2f}")
    print(f"‚úÖ Data points: {trend.data_points}")
    
    # Test idea comparison
    print("\n5. Testing idea comparison...")
    if len(test_ideas) >= 2:
        comparison = engine.compare_ideas(test_ideas[0][0], test_ideas[1][0])
        print(f"‚úÖ Comparison winner: {comparison.winner}")
        print(f"‚úÖ Comparison confidence: {comparison.confidence:.2f}")
        print(f"‚úÖ Insights generated: {len(comparison.insights)}")
    
    # Test performance metrics
    print("\n6. Testing performance metrics...")
    metrics = engine.calculate_performance_metrics()
    print(f"‚úÖ Total ideas evaluated: {metrics.total_ideas_evaluated}")
    print(f"‚úÖ Average score: {metrics.average_score}")
    print(f"‚úÖ System confidence: {metrics.system_confidence}")
    
    # Test insights report
    print("\n7. Testing insights report...")
    insights = engine.generate_insights_report(days=30)
    if "error" not in insights:
        print(f"‚úÖ Insights generated for {insights['period_summary']['total_ideas']} ideas")
        print(f"‚úÖ Trends analyzed: {len(insights['trends'])}")
        print(f"‚úÖ Recommendations: {len(insights['recommendations'])}")
    else:
        print(f"‚ö†Ô∏è Insights error: {insights['error']}")
    
    # Clean up
    engine.close()
    
    print("\nüéâ Historical analysis engine test completed successfully!")
    
    return insights

if __name__ == "__main__":
    # Run historical analysis engine tests
    test_historical_analysis_engine()

